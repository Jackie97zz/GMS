% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/GMS.R
\name{GMS}
\alias{GMS}
\title{Generative Multiple-purpose Sampler}
\usage{
GMS(X = NULL, y, eta_cand = NULL, model, p = NULL, type,
cv_K = 10, sub_size = NULL, S = 100, K0 = 200,  NN_type="WML-MLP",
lr0 = 0.0005, lr_power = 0.2, num_it = 10000, hidden_size = NULL,
L = 4, gpu_ind = 0, custom_loss_file = NULL, custom_penalty_file = NULL,
lam_schd = NULL, save_on = 0, save_path = NULL, batchnorm_on = 0,
sgd = "Adam", verb = 2, intercept = FALSE, penalty = "L1")
}
\arguments{
\item{X}{predictor variable.}

\item{y}{response variable.}

\item{eta_cand}{a candidate set of eta.}

\item{model}{the model of interest. Linear model: "linear"; logistic regression models: "logistic"; least absolute deviation regression: "LAD" quantile regression: "quantile".}

\item{type}{type of GMS. "DoubleBoot": double bootstrap; "CV": cross-validation; "StabSel": stability selection. "SingleBoot" can be used, but "DoubleBoot" type also learns the single bootstrap. We recommend to use "DoubleBoot", instead of "SingleBoot".}

\item{cv_K}{the number of folds for the CV.}

\item{sub_size}{subsampling size when the data size is extremely large.}

\item{S}{the number of subgroups in the bootstrap.}

\item{K0}{update sample size at each iteration.}

\item{NN_type}{the type of neural network generator. "MLP": feed-forwarding NN; "Hadamard": feed-forwarding NNHadamard producted with the bootstrap weights at the last layer; "lowrank": low rank parameterization.}

\item{lr0}{learning rate.}

\item{lr_power}{power rate of learning decay.}

\item{num_it}{the number of iterations.}

\item{hidden_size}{the number of hidden neurons at each layer.}

\item{L}{the number of hidden layers.}

\item{gpu_ind}{gpu index to run the computation. Useful under multiple GPU environment.}

\item{custom_loss_file}{python file that defines a customized loss function of interest. The name of the loss function should be defined as "Loss_func".
It should input X, y, generated theta's, and the dimension of the generated theta should be K0 X p, where K0 is the number of Monte Carlo samples at each iteration.
The ouput of the loss function should be n X K0. Each column is individual losses of observations
for a bootstrap weight; w_i*L(theta; y_i, X_i)_i=1,...,n. An example of a linear regression is \cr
def Loss_func(y, X, Theta): c = torch.matmul(X,Theta.t()) out = (y - c)**2 return out \cr
For a logistic regression, \cr
def Loss_func(y, X, Theta): c = torch.matmul(X,Theta.t()) c = torch.clamp(c, -50.0, 50.0) out = (1-y)*c + torch.log(1.0+torch.exp(-c)) return out.}

\item{custom_penalty_file}{python file that defines a customized penalty
function of interest. Default is L1 penalty. The name of the penalty
function should be defined as "Penalty", and the penalty funciton
inputs a K0 X p dimensional Theta and outputs K0 X 1 dimensional
penalty values. For example of L1 penalty, \cr
def Penalty(Theta): k0 = Theta.size()[0] pen = torch.abs(Theta).sum(1).reshape(k0,1) return pen}

\item{lam_schd}{a candidate set of the tuning parameter of the penalty.}

\item{save_on}{save the trained generator function. 1: save; 0: no save. Default is 0.}

\item{save_path}{the directory path where the trained generator function will be saved.}

\item{batchnorm_on}{batch normalization is used.}

\item{verb}{verb=1: print all information during training every 100 iterations; verb=0: minimal information.}
}
\description{
Train the generator of GMS. You need to install python (>=3.7) and pytorch in advance.
For the details of installation, visit https://pytorch.org/get-started/locally/.
This R package is a wrapepr of python (using pytorch) programs via "reticulate" R package.
}
\examples{
#### linear regression example
library(reticulate)
set.seed(82941)
n = 500;p = 50
bt0 = seq(-1,1,length.out = p)
X = matrix(rnorm(n*p),n,p)
mu0 = crossprod(t(X),bt0)
y = mu0 + rnorm(n)
fit = lm(y~0+X)
theta_hat = fit$coefficients
##############################
#### Training steps
#fit_GMS = GMS(X, y, model="linear", type="DoubleBoot", NN_type="WM-MLP")
#samples_GMS = GMS_Sampling(fit_GMS, B1 = 1000, B10 = 500, X = X, y = y)
#res = post_process(samples_GMS, theta_hat = theta_hat, thre=0.001)
#par(mfrow=c(2,2),mai=c(0.4,0.4,0.1,0.1))
#for(k in 1:4){
#  plot(1:p, type="n", ylim=c(-2.5,2))
#  CI = res[[k+1]]
#  points(bt0, pch=4,col="blue")
#  for(j in 1:p){
#    lines(rep(j,2), c(CI[j,1],CI[j,2]), col="red",lwd=2)
#  }
#  cov = 100*length(which(CI[,1]<bt0 & CI[,2]>bt0))/p
#  text(p/2,-2, paste(cov,"\%",sep=""))
#}
#### Logistic regression example
library(reticulate)
set.seed(82941)
n = 500;p = 30;S = 100
bt0 = seq(-3, 3,length.out = p)
B1 = 3000;B2 = 100;B10 = 100;alpha0 = 0.95;n_b2 = n/S
X = matrix(rnorm(n*p),n,p)
for(j in 1:p){X[,j] = (X[,j] + rnorm(n))/2}
mu = X\%*\%bt0
prob = 1/(1+exp(-1*mu))
y = matrix(rbinom(n,1,prob), n, 1)
fit_GMS = GMS(X, y, model="logistic", type="DoubleBoot", num_it=25000,
lr_power=0.2, L=4, S=100, lr0=0.0001, sgd="Adam", hidden_size=1000,
NN_type="WM-MLP")
samples_GMS = GMS_Sampling(fit_GMS, B1=B1, B2=B2, B10=B10, X=X, y=y,
type="DoubleBoot", gpu_ind=0)
theta_hat = generator(fit_GMS, w=matrix(1,1,S), verb=0)
res = post_process(samples_GMS, alpha=alpha0, theta_hat=theta_hat)
}
\seealso{
\code{\link{GMS_Sampling}}, \code{\link{post_process}}, \code{\link{GMS_Loading}}, \code{\link{generator}}
}
\author{
Minsuk Shin, Jun Liu and Shijie Wang
}
